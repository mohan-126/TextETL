{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wjurQMZkVjgi"},"outputs":[],"source":["from pyspark.sql import SparkSession, Row\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","import re\n","\n","spark = SparkSession.builder.appName(\"NewsETL\").getOrCreate()\n","\n","with open(r\"/content/talk.politics.mideast.txt\", \"r\", errors=\"ignore\") as f:\n","    a = f.read()\n","\n","a = re.sub(r\"\\)\\s*\\n\\s*writes:\", \") writes:\", a, flags=re.IGNORECASE)\n","lines = a.splitlines()\n","\n","records = []\n","record = {}\n","body_lines = []\n","collect_body = False\n","\n","\n","\n","for line in lines:\n","    if line.startswith(\"Newsgroup:\"):\n","        #  previous record\n","        if record:\n","            if \"In article\" not in record:\n","                record[\"In article\"] = None\n","            if \"Newsgroup\" not in record:\n","                record[\"Newsgroup\"] = None\n","            if \"document_id\" not in record:\n","                record[\"document_id\"] = None\n","            if \"From\" not in record:\n","                record[\"From\"] = None\n","            if \"Subject\" not in record:\n","                record[\"Subject\"] = None\n","\n","            record[\"Text\"] = \" \".join(body_lines).strip()\n","            records.append(record)\n","            record, body_lines = {}, []\n","            collect_body = False\n","\n","        record[\"Newsgroup\"] = line.replace(\"Newsgroup:\", \"\").strip()\n","        collect_body = True\n","\n","    elif line.startswith(\"document_id:\"):\n","        record[\"document_id\"] = line.replace(\"document_id:\", \"\").strip()\n","        collect_body = True\n","\n","    elif line.startswith(\"From:\"):\n","        record[\"From\"] = line.replace(\"From:\", \"\").strip()\n","        collect_body = True\n","\n","    elif line.startswith(\"Subject:\"):\n","        record[\"Subject\"] = line.replace(\"Subject:\", \"\").strip()\n","        collect_body = True\n","\n","    elif re.search(r\"(^>*\\s*in article|^>*\\s*in\\s*<)\", line.lower()) or line.endswith(\"writes:\") or line.endswith(\"writes...\"):\n","      if \"In article\" not in record or record[\"In article\"] is None:\n","        record[\"In article\"] = \"\"\n","      if record[\"In article\"]:\n","        record[\"In article\"] += \" \" + line.strip()\n","      else:\n","        record[\"In article\"] = line.strip()\n","        collect_body = True\n","    else:\n","        if collect_body:\n","            body_lines.append(line)\n","\n","#  last record\n","if record:\n","    if \"In article\" not in record:\n","        record[\"In article\"] = None\n","    if \"Newsgroup\" not in record:\n","        record[\"Newsgroup\"] = None\n","    if \"document_id\" not in record:\n","        record[\"document_id\"] = None\n","    if \"From\" not in record:\n","        record[\"From\"] = None\n","    if \"Subject\" not in record:\n","        record[\"Subject\"] = None\n","    record[\"Text\"] = \" \".join(body_lines).strip()\n","    records.append(record)\n","\n","\n","# Create DataFrame with explicit schema\n","schema = StructType([\n","    StructField(\"Newsgroup\", StringType(), True),\n","    StructField(\"document_id\", StringType(), True),\n","    StructField(\"From\", StringType(), True),\n","    StructField(\"Subject\", StringType(), True),\n","    StructField(\"In_article\", StringType(), True),\n","    StructField(\"Text\", StringType(), True)\n","])\n","\n","# Create DataFrame\n","df_clean = spark.createDataFrame([Row(**rec) for rec in records], schema=schema)\n","\n","#print(\"Clean DataFrame schema:\")\n","#df_clean.printSchema()\n","#print(\"Clean DataFrame sample:\")\n","#df_clean.show(2, truncate=False)\n","\n","df_clean.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/bronze_clean\")\n","\n","df_bronze_clean = spark.read.format(\"parquet\").load(\"/content/bronze_clean\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"kzYeiiZMIluK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession, Row\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","import re\n","\n","spark = SparkSession.builder.appName(\"NewsETL\").getOrCreate()\n","\n","with open(r\"/content/talk.politics.mideast.txt\", \"r\", errors=\"ignore\") as f:\n","    a = f.read()\n","\n","a = re.sub(r\"\\)\\s*\\n\\s*writes:\", \") writes:\", a, flags=re.IGNORECASE)\n","lines = a.splitlines()\n","\n","records = []\n","record = {}\n","body_lines = []\n","collect_body = False\n","\n","for i, line in enumerate(lines):\n","    line = line.strip()\n","\n","    if not line:\n","        continue\n","\n","    if line.startswith(\"Newsgroup:\"):\n","        # previous record\n","        if record:\n","            if \"In article\" not in record:\n","                record[\"In article\"] = None\n","            if \"Newsgroup\" not in record:\n","                record[\"Newsgroup\"] = None\n","            if \"document_id\" not in record:\n","                record[\"document_id\"] = None\n","            if \"From\" not in record:\n","                record[\"From\"] = None\n","            if \"Subject\" not in record:\n","                record[\"Subject\"] = None\n","\n","            record[\"Text\"] = \" \".join(body_lines).strip()\n","            records.append(record)\n","            record, body_lines = {}, []\n","            collect_body = False\n","\n","        record[\"Newsgroup\"] = line.replace(\"Newsgroup:\", \"\").strip()\n","        collect_body = True\n","\n","    elif line.startswith(\"document_id:\"):\n","        record[\"document_id\"] = line.replace(\"document_id:\", \"\").strip()\n","        collect_body = True\n","\n","    elif line.startswith(\"From:\"):\n","        record[\"From\"] = line.replace(\"From:\", \"\").strip()\n","        collect_body = True\n","\n","    elif line.startswith(\"Subject:\"):\n","        record[\"Subject\"] = line.replace(\"Subject:\", \"\").strip()\n","        collect_body = True\n","\n","    elif re.search(r\"(^>*\\s*in article|^>*\\s*in\\s*<)\", line.lower()) or line.endswith(\"writes:\") or line.endswith(\"writes...\"):\n","        if \"In article\" not in record or record[\"In article\"] is None:\n","            record[\"In article\"] = \"\"\n","        if record[\"In article\"]:\n","            record[\"In article\"] += \" \" + line.strip()\n","        else:\n","            record[\"In article\"] = line.strip()\n","        collect_body = True\n","    else:\n","        if collect_body:   # collect body lines\n","            body_lines.append(line)\n","\n","# last record\n","if record:\n","    if \"In article\" not in record:\n","        record[\"In article\"] = None\n","    if \"Newsgroup\" not in record:\n","        record[\"Newsgroup\"] = None\n","    if \"document_id\" not in record:\n","        record[\"document_id\"] = None\n","    if \"From\" not in record:\n","        record[\"From\"] = None\n","    if \"Subject\" not in record:\n","        record[\"Subject\"] = None\n","    record[\"Text\"] = \" \".join(body_lines).strip()\n","    records.append(record)\n","\n","schema = StructType([\n","    StructField(\"Newsgroup\", StringType(), True),\n","    StructField(\"document_id\", StringType(), True),\n","    StructField(\"From\", StringType(), True),\n","    StructField(\"Subject\", StringType(), True),\n","    StructField(\"In_article\", StringType(), True),\n","    StructField(\"Text\", StringType(), True)\n","])\n","\n","# Create DataFrame\n","df_clean = spark.createDataFrame([Row(**rec) for rec in records], schema=schema)\n","\n","#print(\"Clean DataFrame schema:\")\n","#df_clean.printSchema()\n","#print(\"Clean DataFrame sample:\")\n","#df_clean.show(2, truncate=False)\n","\n","df_clean.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/ETL/bronze_clean\")\n","\n","df_bronze_clean = spark.read.format(\"parquet\").load(\"/content/ETL/bronze_clean\")\n","\n"],"metadata":{"id":"I1nLK3SyAVuC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, regexp_replace, trim\n","\n","df_silver_clean = df_bronze_clean.withColumn    (\"In_article\",coalesce(col(\"In_article\"), lit(\"NA\"))).withColumn(\n","    \"In_articles_clean\",\n","    trim(\n","        regexp_replace(\n","            col(\"In_article\"),\n","            r\"(?i)(>+|\\|>|>IN|>>IN article|>\\||#|\\s*In article\\s*|writes:|writes\\.\\.\\.)\",\n","            \"\"\n","        )\n","    )\n",").withColumn(\n","    \"Text_clean\",\n","    trim(\n","        regexp_replace(\n","            col(\"Text\"),\n","            r\"(>+|\\s*>+\\s*|\\|>|>\\||\\^|#|:+)\",\n","            \"\"\n","        )\n","    )\n",").withColumn(\"Data_ingested_date\",current_timestamp())\n","\n","df_silver_clean.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/ETL/silver_clean\")\n","\n","#df_silver.select(\"document_id\", \"In_article\", \"In_articles_clean\").show(100,truncate=False)\n","#df_silver.select(\"document_id\", \"Text\", \"Text_clean\",).show(1,truncate=False)\n","#df_silver_clean.show(1,truncate=False)\n","#df_silver.filter(col(\"document_id\")==\"75886\").select(\"document_id\", \"Text\", \"Text_clean\",).show(100,truncate=False)\n","#df_silver.filter (col(\"In_article\")==\"NA\").count()\n","\n","df_silver_clean.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/ETL/Gold\")\n","\n"],"metadata":{"id":"-Cs64R0_FK-X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_gold = spark.read.format(\"parquet\").load(\"/content/ETL/Gold\")\n","df_gold.show(1,truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QP5v1RuYQKeI","executionInfo":{"status":"ok","timestamp":1756444600984,"user_tz":-330,"elapsed":2492,"user":{"displayName":"mohan","userId":"07768512104687946224"}},"outputId":"e9821123-e841-4c6d-a905-3bd448fe4d95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------+-----------+-----------------------+-----------------------------------------------------+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+\n","|Newsgroup            |document_id|From                   |Subject                                              |In_article                                                                     |Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |In_articles_clean                                          |Text_clean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Data_ingested_date        |\n","+---------------------+-----------+-----------------------+-----------------------------------------------------+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+\n","|talk.politics.mideast|75364      |hasan@McRCIM.McGill.EDU|Re: ISLAM BORDERS. ( was :Israel: misisipi to ganges)|In article <4805@bimacs.BITNET>, ehrlich@bimacs.BITNET (Gideon Ehrlich) writes:||> |> Hassan and some other seemed not to be a ware that Jews celebrating on |> these days Thje Passover holliday the holidy of going a way from the |> Nile. |> So if one let his imagination freely work it seemed beter to write |> that the Zionist drean is \"from the misisipi to the Nile \". the question is by going East or West from the misisipi. on either choice you would loose Palestine or Broklyn, N.Y. I thought you're gonna say fromn misisipi back to the misisipi ! |> By the way : |> |> What are the borders the Islamic world dreams about ?? |> |> Islamic readers, I am waiting to your honest answer. Let's say : \" let's establish the islamic state first\" or \"let's free our occupied lands first\". And then we can dream about expansion, Mr. Gideon hasan|<4805@bimacs.BITNET, ehrlich@bimacs.BITNET (Gideon Ehrlich)|Hassan and some other seemed not to be a ware that Jews celebrating on  these days Thje Passover holliday the holidy of going a way from the  Nile.  So if one let his imagination freely work it seemed beter to write  that the Zionist drean is \"from the misisipi to the Nile \". the question is by going East or West from the misisipi. on either choice you would loose Palestine or Broklyn, N.Y. I thought you're gonna say fromn misisipi back to the misisipi !  By the way    What are the borders the Islamic world dreams about ??   Islamic readers, I am waiting to your honest answer. Let's say  \" let's establish the islamic state first\" or \"let's free our occupied lands first\". And then we can dream about expansion, Mr. Gideon hasan|2025-08-29 05:16:26.035493|\n","+---------------------+-----------+-----------------------+-----------------------------------------------------+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+\n","only showing top 1 row\n","\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPcvebFf/4RS6fP4dFTCIcS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}